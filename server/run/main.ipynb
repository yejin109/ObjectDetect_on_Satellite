{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[{"file_id":"1t5Take8Jk6KPSpEOhcUblnhN5KGqRbyT","timestamp":1658704171163}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["reference : https://pseudo-lab.github.io/Tutorial-Book/chapters/object-detection/Ch5-Faster-R-CNN.html"],"metadata":{"id":"oJYyAXLDvODO"}},{"cell_type":"markdown","source":["# package"],"metadata":{"id":"nOSkklYveplK"}},{"cell_type":"code","source":[""],"metadata":{"id":"Q6biwB0vkhr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSxode6pT7l5"},"outputs":[],"source":["import time\n","import os\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.patches import Rectangle\n","%matplotlib inline\n","\n","import natsort\n","import random\n","import shutil\n","\n","from bs4 import BeautifulSoup\n","from PIL import Image"]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')          \n","else:\n","    device = torch.device('cpu')\n","    \n","print('device:', device)"],"metadata":{"id":"T31vcz63WE_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661305373502,"user_tz":-540,"elapsed":341,"user":{"displayName":"한이음인공위성","userId":"00807510854728857692"}},"outputId":"261e1216-cdf7-47ff-a3b0-fd27079b0971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device: cpu\n"]}]},{"cell_type":"markdown","source":["# path 설정"],"metadata":{"id":"O6HgcEGreY2p"}},{"cell_type":"markdown","source":["# mount"],"metadata":{"id":"uZrZWJp0eiYz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"x-75nMVUYXv-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661305407607,"user_tz":-540,"elapsed":30202,"user":{"displayName":"한이음인공위성","userId":"00807510854728857692"}},"outputId":"69333117-6945-4ca0-eeec-a39aa6fdb0c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# config"],"metadata":{"id":"tsDNeN9UewsK"}},{"cell_type":"code","source":["#데이터 분리가 되어있는 경우\n","\n","dir_a = r\"/data/train\"   #edit\n","dir_i = r\"드라이브 경로/images/train\"  #edit\n","testdir_a = r\"드라이브 경로/annotations/test\"  #edit\n","testdir_i = r\"드라이브 경로/images/test\"  #edit\n","\n","annotations = natsort.natsorted(os.listdir(dir_a))\n","images = natsort.natsorted(os.listdir(dir_i))\n","test_annotations = natsort.natsorted(os.listdir(testdir_a))\n","test_images = natsort.natsorted(os.listdir(testdir_i))\n","\n","print(len(annotations))\n","print(len(images))\n","print(len(test_annotations))\n","print(len(test_images))\n","\n","print(annotations)\n","print(images)\n","print(test_annotations)\n","print(test_images)"],"metadata":{"id":"MxySHuKPfcCr","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1661305567712,"user_tz":-540,"elapsed":391,"user":{"displayName":"한이음인공위성","userId":"00807510854728857692"}},"outputId":"f013850a-11d7-475e-afa3-e3acf9320f4f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-17fdd5c9676f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtestdir_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"드라이브 경로/images/test\"\u001b[0m  \u001b[0;31m#edit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdir_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/train'"]}]},{"cell_type":"code","source":["#데이터 분리를 수행하는 경우\n","\n","annotations = r\"드라이브 경로\"   #labeling data\n","images = r\"드라이브 경로\"  #raw data\n","\n","print(len(os.listdir(annotations)))\n","print(len(os.listdir(images)))\n","\n","!mkdir test_images\n","!mkdir test_annotations\n","\n","random.seed(1234)\n","idx = random.sample(range(1748), 500)\n","\n","for img in np.array(sorted(os.listdir(images)))[idx]:\n","    shutil.move(images, '드라이브 경로/test_images/')\n","\n","for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n","    shutil.move(annotations, '드라이브 경로/test_annotations/')\n","\n","test_annotations = r\"드라이브 경로/test_images/\"\n","test_images = r\"드라이브 경로/test_annotations/\"\n","\n","print(len(os.listdir(test_annotations)))\n","print(len(os.listdir(test_images)))"],"metadata":{"id":"orh_z0cQWcio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# setup"],"metadata":{"id":"u5n6LJwFe43j"}},{"cell_type":"markdown","source":["# model"],"metadata":{"id":"tS-oLzhdoPII"}},{"cell_type":"code","source":["def get_model_instance_segmentation(num_classes):\n","  \n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model"],"metadata":{"id":"S9c-ViAFoU2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = get_model_instance_segmentation('num_classes에 해당하는 숫자 지정')  #edit\n","\n","model.to(device)"],"metadata":{"id":"WIfU9znkospT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"HjTTYaNjo9ye"}},{"cell_type":"code","source":["num_epochs = #edit 해당하는 숫자 넣기\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)  #optimizer 종류, lr, momentum, weight_decay 모두 변경 가능"],"metadata":{"id":"9a_Is_N2o_x1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch, num_epochs, model, optimizer, scheduler):\n","\n","  for epoch in range(num_epochs):\n","    start = time.time()\n","    model.train()\n","    i = 0    \n","    epoch_loss = 0\n","    batch_loss_list = []\n","    progress = ProgressMonitor(length=len(train_dataset)) #train_dataset raw images 담긴 폴더로\n","\n","    for imgs, annotations in data_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs) #raw image 저장한 것 찾아서 마지막 imgs 대체\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations] #labeling data 저장한 것 찾아서 마지막 annotations 대체\n","        loss_dict = model(imgs, annotations) \n","        losses = sum(loss for loss in loss_dict.values())        \n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step() \n","        epoch_loss += losses\n","\n","        batch_loss_list.append(loss.item())\n","        progress.update(epoch, num_epochs, batch.shape[0], sum(batch_loss_list)/len(batch_loss_list) )\n","\n","    if scheduler:\n","        scheduler.step()\n","\n","    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"],"metadata":{"id":"BbK5iFi2pZYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#torch.save(model.state_dict(),f'model_{num_epochs}.pt')\n","#model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))"],"metadata":{"id":"7hX-Y_P9rqBK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"x8KpREsutDZ_"}},{"cell_type":"code","source":["#시험 데이터 하나에 대해 예측\n","\n","def make_prediction(model, img, threshold):\n","    model.eval()\n","    preds = model(img)\n","    for id in range(len(preds)) :\n","        idx_list = []\n","\n","        for idx, score in enumerate(preds[id]['scores']) :\n","            if score > threshold : \n","                idx_list.append(idx)\n","\n","        preds[id]['boxes'] = preds[id]['boxes'][idx_list]  #바운딩 박스 좌표\n","        preds[id]['labels'] = preds[id]['labels'][idx_list]  #클래스\n","        preds[id]['scores'] = preds[id]['scores'][idx_list]  #점수\n","\n","    return preds"],"metadata":{"id":"API9ahrKtGtX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad(): \n","    # 테스트셋 배치사이즈= 2\n","    for imgs, annotations in test_data_loader:\n","        imgs = list(img.to(device) for img in imgs)\n","\n","        pred = make_prediction(model, imgs, 0.5) #0.5 이상인 신뢰도 값만 저장 >변경 가능\n","        print(pred)\n","        break"],"metadata":{"id":"eAQf-E20ti4N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 기타"],"metadata":{"id":"hta_s8eJfGyE"}},{"cell_type":"code","source":["#시각화\n","\n","_idx = 1\n","print(\"Target : \", annotations[_idx]['labels'])\n","plot_image_from_output(imgs[_idx], annotations[_idx])\n","print(\"Prediction : \", pred[_idx]['labels'])\n","plot_image_from_output(imgs[_idx], pred[_idx])"],"metadata":{"id":"0vsJTOm4uiig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#전체 시험 데이터에 대해 예측\n","\n","from tqdm import tqdm\n","\n","labels = []\n","preds_adj_all = [] #예측 결과 담을 list\n","annot_all = [] #label 담을 list\n","\n","for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n","    im = list(img.to(device) for img in im)\n","    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n","\n","    for t in annot:\n","        labels += t['labels']\n","\n","    with torch.no_grad():\n","        preds_adj = make_prediction(model, im, 0.5) #0.5 이상인 신뢰도 값만 저장 >변경 가능\n","        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n","        preds_adj_all.append(preds_adj)\n","        annot_all.append(annot)"],"metadata":{"id":"G3nR2ha9ua41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd Tutorial-Book-Utils/\n","import utils_ObjectDetection as utils"],"metadata":{"id":"j3ApI0DIvG_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_metrics = []\n","for batch_i in range(len(preds_adj_all)):\n","    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n","\n","true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n","precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n","mAP = torch.mean(AP)\n","print(f'mAP : {mAP}')\n","print(f'AP : {AP}')"],"metadata":{"id":"xwO0QDNFvIi2"},"execution_count":null,"outputs":[]}]}